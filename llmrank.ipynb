{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup config\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,  # default to WARNING\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SlopRankLogger\")\n",
    "logger.setLevel(logging.INFO)  # Our SlopRank logs at INFO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration (EvalConfig)\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    \"\"\"Configuration for the evaluation system.\"\"\"\n",
    "    model_names: List[str]\n",
    "    evaluation_method: int  # e.g., 1 => numeric rating\n",
    "    use_subset_evaluation: bool\n",
    "    evaluators_subset_size: int\n",
    "    output_dir: Path\n",
    "    request_delay: float = 0.0  # adjustable delay between requests if needed\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        if self.evaluation_method not in {1, 2}:\n",
    "            raise ValueError(\"evaluation_method must be 1 or 2\")\n",
    "        if self.evaluators_subset_size >= len(self.model_names):\n",
    "            raise ValueError(\"evaluators_subset_size must be < number of models\")\n",
    "\n",
    "DEFAULT_CONFIG = EvalConfig(\n",
    "    model_names=[\n",
    "        \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "        \"gemini-exp-1206\",\n",
    "        \"claude-3-5-sonnet-latest\",\n",
    "        \"o1-preview\",\n",
    "        \"gpt-4o\",\n",
    "        \"deepseek-chat\",\n",
    "        # \"groq-llama-3.3-70b\"\n",
    "    ],\n",
    "    evaluation_method=1,  # numeric\n",
    "    use_subset_evaluation=True,\n",
    "    evaluators_subset_size=3,\n",
    "    output_dir=Path(\"results\"),  # folder for CSV outputs\n",
    "    request_delay=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 23:10:50,274 - INFO - Reading prompts from prompts.xlsx ...\n",
      "2025-01-13 23:10:50,290 - INFO - Loaded 37 prompts from Excel.\n"
     ]
    }
   ],
   "source": [
    "# 3. Read prompts\n",
    "# You should have a local \"prompts.xlsx\" file with columns [\"Questions\", \"Answer_key\"].\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # if you have .env credentials\n",
    "\n",
    "logger.info(\"Reading prompts from prompts.xlsx ...\")\n",
    "prompts_df = pd.read_excel(\"prompts.xlsx\", sheet_name=0)\n",
    "prompts = prompts_df[\"Questions\"].tolist()\n",
    "\n",
    "# If \"Answer_key\" column exists, read it; otherwise fallback to None\n",
    "if \"Answer_key\" in prompts_df.columns:\n",
    "    answer_keys = prompts_df[\"Answer_key\"].tolist()\n",
    "else:\n",
    "    logger.warning(\"No Answer_key column found; using None.\")\n",
    "    answer_keys = [None]*len(prompts_df)\n",
    "\n",
    "prompt_pairs = list(zip(prompts, answer_keys))\n",
    "logger.info(f\"Loaded {len(prompt_pairs)} prompts from Excel.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Collecting the responses (with partial checks)\n",
    "\n",
    "def collect_responses(prompt_pairs: List[Tuple[str, str]], config: EvalConfig, llm_module) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query each model with each prompt, skipping any (prompt, model) pairs\n",
    "    already found in the existing responses.csv. \n",
    "    Return the combined DataFrame: (prompt, model, response, is_valid, response_time, Answer_key, token_count).\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Collecting responses (with partial coverage check)...\")\n",
    "\n",
    "    # 1) Try to load existing responses\n",
    "    resp_path = config.output_dir / \"responses.csv\"\n",
    "    existing_responses_df = None\n",
    "    if resp_path.exists():\n",
    "        logger.info(f\"Found existing responses at {resp_path}, will skip duplicates.\")\n",
    "        existing_responses_df = pd.read_csv(resp_path)\n",
    "    else:\n",
    "        logger.info(\"No existing responses file found; we'll collect everything from scratch.\")\n",
    "\n",
    "    new_rows = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    # 2) For each (prompt, answer_key) pair\n",
    "    for i, (prompt, answer_key) in enumerate(prompt_pairs, 1):\n",
    "        logger.info(f\"Processing prompt {i}/{len(prompt_pairs)}: {prompt[:60]}...\")\n",
    "        for model_name in config.model_names:\n",
    "            # Skip if we already have a row for (prompt, model_name)\n",
    "            if existing_responses_df is not None:\n",
    "                subset = existing_responses_df[\n",
    "                    (existing_responses_df[\"prompt\"] == prompt) &\n",
    "                    (existing_responses_df[\"model\"] == model_name)\n",
    "                ]\n",
    "                if not subset.empty:\n",
    "                    # Already have it; skip\n",
    "                    logger.info(f\"Skipping existing response for model={model_name}, prompt={prompt[:40]}...\")\n",
    "                    continue\n",
    "\n",
    "            # Otherwise, query the model now\n",
    "            start_time = time.time()\n",
    "            logger.info(f\"Querying {model_name} for new response...\")\n",
    "            try:\n",
    "                model = llm_module.get_model(model_name)\n",
    "                raw_response = model.prompt(prompt).text()\n",
    "\n",
    "                valid = isinstance(raw_response, str) and len(raw_response.strip()) >= 10\n",
    "                elapsed = time.time() - start_time\n",
    "                tokens_used = len(raw_response.split())\n",
    "\n",
    "                new_rows.append({\n",
    "                    'prompt': prompt,\n",
    "                    'model': model_name,\n",
    "                    'response': raw_response if valid else None,\n",
    "                    'is_valid': valid,\n",
    "                    'response_time': elapsed,\n",
    "                    'Answer_key': answer_key,\n",
    "                    'token_count': tokens_used\n",
    "                })\n",
    "                logger.info(\n",
    "                    f\"{model_name} responded in {elapsed:.2f}s - {'Valid' if valid else 'Invalid'}\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                elapsed = time.time() - start_time\n",
    "                logger.error(f\"Error from {model_name} after {elapsed:.2f}s: {str(e)}\")\n",
    "                new_rows.append({\n",
    "                    'prompt': prompt,\n",
    "                    'model': model_name,\n",
    "                    'response': None,\n",
    "                    'is_valid': False,\n",
    "                    'response_time': elapsed,\n",
    "                    'Answer_key': answer_key,\n",
    "                    'token_count': 0\n",
    "                })\n",
    "\n",
    "            if config.request_delay > 0.0:\n",
    "                time.sleep(config.request_delay)\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    logger.info(f\"Response collection done in {total_time:.2f}s\")\n",
    "\n",
    "    # 3) Combine with existing responses if any\n",
    "    if existing_responses_df is not None:\n",
    "        new_df = pd.DataFrame(new_rows)\n",
    "        combined_df = pd.concat([existing_responses_df, new_df], ignore_index=True)\n",
    "        # Drop duplicates if needed\n",
    "        combined_df.drop_duplicates(subset=[\"prompt\", \"model\"], keep=\"first\", inplace=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        # No prior file => just return new rows\n",
    "        return pd.DataFrame(new_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Collecting Raw Evaluations (Unparsed), with partial checks\n",
    "\n",
    "def collect_raw_evaluations(responses_df: pd.DataFrame, config: EvalConfig, llm_module) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Each model in config.model_names evaluates (rates) the others' responses\n",
    "    but we skip if we already have a row for (prompt, judge_model, model_mapping) \n",
    "    in raw_evaluations.csv. \n",
    "    Returns the combined DataFrame of new + old.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Collecting raw evaluations (unparsed, partial check)...\")\n",
    "\n",
    "    # 1) Try loading existing raw evaluations\n",
    "    raw_eval_path = config.output_dir / \"raw_evaluations.csv\"\n",
    "    existing_raw_eval_df = None\n",
    "    if raw_eval_path.exists():\n",
    "        logger.info(f\"Found existing raw evaluations at {raw_eval_path}, will skip duplicates.\")\n",
    "        existing_raw_eval_df = pd.read_csv(raw_eval_path)\n",
    "    else:\n",
    "        logger.info(\"No existing raw_evaluations.csv found; collecting from scratch.\")\n",
    "\n",
    "    new_judgments = []\n",
    "\n",
    "    unique_prompts = responses_df['prompt'].unique()\n",
    "    for prompt in unique_prompts:\n",
    "        prompt_subset = responses_df[responses_df['prompt'] == prompt]\n",
    "        answer_key = prompt_subset['Answer_key'].iloc[0] if 'Answer_key' in prompt_subset.columns else None\n",
    "        prompt_responses = prompt_subset.set_index('model')['response'].to_dict()\n",
    "\n",
    "        # Evaluate with each model as judge\n",
    "        for judge_model in config.model_names:\n",
    "            # Exclude judge's own or missing responses\n",
    "            other_models = [\n",
    "                m for m in config.model_names\n",
    "                if m != judge_model and prompt_responses.get(m) is not None\n",
    "            ]\n",
    "            if config.use_subset_evaluation and other_models:\n",
    "                other_models = random.sample(\n",
    "                    other_models,\n",
    "                    min(config.evaluators_subset_size, len(other_models))\n",
    "                )\n",
    "\n",
    "            if not other_models:\n",
    "                continue\n",
    "\n",
    "            # Build the anonymized mapping\n",
    "            model_to_anon = {m: f\"Model_{i+1}\" for i, m in enumerate(other_models)}\n",
    "            answers_section = \"\\n\".join([\n",
    "                f\"{model_to_anon[m]}:\\n{prompt_responses[m]}\\n---\"\n",
    "                for m in other_models\n",
    "            ])\n",
    "            if answer_key:\n",
    "                answer_key_edited = f\"The Answer Key here is:\\n{answer_key}\\n---\\n\"\n",
    "            else:\n",
    "                answer_key_edited = \"\"\n",
    "\n",
    "            instructions = f\"\"\"\n",
    "You are an expert evaluator tasked with assessing the quality of responses from different language models. Your goal is to provide accurate and unbiased ratings based on a given problem, answer key, and set of criteria.\n",
    "\n",
    "First, carefully read the following information:\n",
    "\n",
    "Here is the original problem or prompt:\n",
    "<problem>\n",
    "{prompt}\n",
    "</problem>\n",
    "\n",
    "Here are the answers provided by different models:\n",
    "<answers_section>\n",
    "{answers_section}\n",
    "</answers_section>\n",
    "\n",
    "Here is the answer key to guide your evaluation. It will tell you what could be considered GOOD and BAD, so you can rate appropriately:\n",
    "<answer_key>\n",
    "{answer_key_edited}\n",
    "</answer_key>\n",
    "\n",
    "Your task is to evaluate the answers provided by all the models (Model_1, Model_2, etc.) based on these criteria:\n",
    "1. Accuracy: How well does the answer align with the information as per the answer key?\n",
    "2. Completeness: Does the answer cover all necessary aspects of the problem?\n",
    "3. Clarity: Is the answer easy to understand and well-structured?\n",
    "4. Relevance: Does the answer directly address the given problem?\n",
    "\n",
    "For each model, you will provide a rating on a scale of 1 to 10 for each criterion, where:\n",
    "- 10: Exceptional, world-class, zero errors and all the relevant nuances.\n",
    "- 8-9: Excellent, like a top professional in the field. Not perfect though.\n",
    "- 6-7: Good, like a competent undergraduate student. Doesn't stand out. Average.\n",
    "- 4-5: Fair, like an average high school student. Barely satisfactory.\n",
    "- 1-3: Poor. Factually incorrect and wrong logic.\n",
    "\n",
    "Please follow the following process to evaluate each model, in markdown format:\n",
    "1. Read the problem, answer key, and the model's answer carefully.\n",
    "2. For each criterion:\n",
    "   a. Think through the key points from the answer that relate to this criterion.\n",
    "   b. Consider strengths and weaknesses.\n",
    "   c. Provide a score.\n",
    "3. Calculate an overall score.\n",
    "\n",
    "Wrap your detailed evaluation for each model in <detailed_evaluation> tags.\n",
    "\n",
    "Provide your final ratings in a JSON object with the following structure:\n",
    "{{\"Model_1\": X, \"Model_2\": Y}}\n",
    "Where X and Y are integer values between 1 and 10.\n",
    "\n",
    "Remember:\n",
    "- Adhere strictly to the JSON format specified above, i.e., put the response inside a curly bracket.\n",
    "- Provide neutral and accurate ratings based solely on the answer key and the given criteria.\n",
    "- Ensure that your evaluation is thorough and justified.\n",
    "\n",
    "Begin your evaluation now.\"\"\".strip()\n",
    "\n",
    "            # 2) If we already have a row for (prompt, judge_model, model_mapping), skip\n",
    "            model_mapping_str = json.dumps(model_to_anon, sort_keys=True)\n",
    "            already_exists = False\n",
    "            if existing_raw_eval_df is not None:\n",
    "                possible_matches = existing_raw_eval_df[\n",
    "                    (existing_raw_eval_df[\"prompt\"] == prompt) &\n",
    "                    (existing_raw_eval_df[\"judge_model\"] == judge_model)\n",
    "                ]\n",
    "                # Now check if any row has the exact same model_mapping\n",
    "                # We sort_keys=True above so that JSON string is consistent\n",
    "                found_match = possible_matches[\n",
    "                    possible_matches[\"model_mapping\"] == model_mapping_str\n",
    "                ]\n",
    "                if not found_match.empty:\n",
    "                    logger.info(f\"Skipping existing raw eval for judge={judge_model}, prompt={prompt[:40]}...\")\n",
    "                    already_exists = True\n",
    "\n",
    "            if already_exists:\n",
    "                continue\n",
    "\n",
    "            # 3) Otherwise, run the LLM judge\n",
    "            try:\n",
    "                judge_llm = llm_module.get_model(judge_model)\n",
    "                judge_result_obj = judge_llm.prompt(instructions)\n",
    "                raw_judgment = judge_result_obj.text()\n",
    "                raw_judgment_tokens = len(raw_judgment.split())\n",
    "\n",
    "                new_judgments.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"raw_judgment\": raw_judgment,\n",
    "                    # store the same sorted string\n",
    "                    \"model_mapping\": model_mapping_str,\n",
    "                    \"raw_judgment_token_count\": raw_judgment_tokens\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error collecting raw eval from judge={judge_model} on prompt='{prompt}': {str(e)}\")\n",
    "                new_judgments.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"raw_judgment\": None,\n",
    "                    \"model_mapping\": model_mapping_str,\n",
    "                    \"raw_judgment_token_count\": 0,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "    new_eval_df = pd.DataFrame(new_judgments)\n",
    "    logger.info(\"Finished collecting new raw evaluation outputs.\")\n",
    "\n",
    "    # 4) Combine with existing, if any\n",
    "    if existing_raw_eval_df is not None and not new_eval_df.empty:\n",
    "        combined_df = pd.concat([existing_raw_eval_df, new_eval_df], ignore_index=True)\n",
    "        combined_df.drop_duplicates(subset=[\"prompt\", \"judge_model\", \"model_mapping\"], keep=\"first\", inplace=True)\n",
    "        return combined_df\n",
    "    elif existing_raw_eval_df is not None:\n",
    "        # No new data, just return old\n",
    "        return existing_raw_eval_df\n",
    "    else:\n",
    "        # Everything is new\n",
    "        return new_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Parsing the raw evaluations\n",
    "\n",
    "def parse_evaluation_rows(raw_eval_df: pd.DataFrame, config: EvalConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse each row of the raw_eval_df, which contains judge's raw JSON-like output.\n",
    "    If parsing fails, fallback to a default rating (4.1) for each rated model.\n",
    "    \n",
    "    Returns a DataFrame: (prompt, judge_model, rated_model, score).\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "\n",
    "    for _, row in raw_eval_df.iterrows():\n",
    "        prompt = row[\"prompt\"]\n",
    "        judge_model = row[\"judge_model\"]\n",
    "        raw_judgment = row[\"raw_judgment\"]\n",
    "        raw_judgment_tokens = row.get(\"raw_judgment_token_count\", 0)\n",
    "\n",
    "        # Convert model_mapping from JSON string back to dict\n",
    "        try:\n",
    "            model_mapping = json.loads(row[\"model_mapping\"])  # e.g. {\"gemini-exp-1206\":\"Model_1\"}\n",
    "        except:\n",
    "            model_mapping = {}\n",
    "\n",
    "        if not raw_judgment:\n",
    "            # If there's no raw judgment at all, we might skip or fallback\n",
    "            for real_model in model_mapping.keys():\n",
    "                evaluations.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"rated_model\": real_model,\n",
    "                    \"score\": 4.1,           # << changed fallback\n",
    "                    \"parse_failed\": True\n",
    "                })\n",
    "            logger.warning(f\"No raw_judgment for prompt={prompt}, judge={judge_model}; skipping parse.\")\n",
    "            continue\n",
    "\n",
    "        # Try to parse a JSON object from the raw_judgment\n",
    "        try:\n",
    "            start = raw_judgment.find(\"{\")\n",
    "            end = raw_judgment.rfind(\"}\") + 1\n",
    "\n",
    "            if start == -1 or end == 0:\n",
    "                raise ValueError(\"No JSON object found in raw_judgment\")\n",
    "\n",
    "            data = json.loads(raw_judgment[start:end])\n",
    "            # Reverse mapping: \"Model_1\" => \"gemini-exp-1206\"\n",
    "            anon_to_real = {v: k for k, v in model_mapping.items()}\n",
    "\n",
    "            for anon_id, score in data.items():\n",
    "                real_model = anon_to_real.get(anon_id)\n",
    "                if not real_model:\n",
    "                    # If we can't find the real model name, skip\n",
    "                    continue\n",
    "                numeric_score = float(score)\n",
    "                numeric_score = max(1.0, min(10.0, numeric_score))  # clamp 1..10\n",
    "\n",
    "                evaluations.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"rated_model\": real_model,\n",
    "                    \"score\": numeric_score,\n",
    "                    \"parse_failed\": False,\n",
    "                    \"raw_judgment_token_count\": raw_judgment_tokens\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Parsing error for judge={judge_model}, prompt={prompt}: {str(e)}\")\n",
    "            # If parse fails, assign a default rating\n",
    "            for real_model in model_mapping.keys():\n",
    "                evaluations.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"rated_model\": real_model,\n",
    "                    \"score\": 4.1,\n",
    "                    \"parse_failed\": True,\n",
    "                    \"raw_judgment_token_count\": raw_judgment_tokens\n",
    "                })\n",
    "\n",
    "    evals_df = pd.DataFrame(evaluations)\n",
    "    return evals_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 23:10:50,326 - INFO - Using config: EvalConfig(model_names=['gemini-2.0-flash-thinking-exp-1219', 'gemini-exp-1206', 'claude-3-5-sonnet-latest', 'o1-preview', 'gpt-4o', 'deepseek-chat'], evaluation_method=1, use_subset_evaluation=True, evaluators_subset_size=3, output_dir=PosixPath('results'), request_delay=0.0)\n",
      "2025-01-13 23:10:50,327 - INFO - No responses.csv found; collecting now from each model.\n",
      "2025-01-13 23:10:50,327 - INFO - Collecting responses (with partial coverage check)...\n",
      "2025-01-13 23:10:50,327 - INFO - No existing responses file found; we'll collect everything from scratch.\n",
      "2025-01-13 23:10:50,327 - INFO - Processing prompt 1/37: Analyze and compare the architectural styles of the Hagia So...\n",
      "2025-01-13 23:10:50,328 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:11:03,623 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 13.29s - Valid\n",
      "2025-01-13 23:11:03,624 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:11:31,639 - INFO - gemini-exp-1206 responded in 28.02s - Valid\n",
      "2025-01-13 23:11:31,641 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:11:46,554 - INFO - claude-3-5-sonnet-latest responded in 14.91s - Valid\n",
      "2025-01-13 23:11:46,555 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:12:06,489 - INFO - o1-preview responded in 19.93s - Valid\n",
      "2025-01-13 23:12:06,490 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:12:16,165 - INFO - gpt-4o responded in 9.68s - Valid\n",
      "2025-01-13 23:12:16,166 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:12:37,824 - INFO - deepseek-chat responded in 21.66s - Valid\n",
      "2025-01-13 23:12:37,825 - INFO - Processing prompt 2/37: What are the characteristics of APOBEC-driven SGMs, particul...\n",
      "2025-01-13 23:12:37,825 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:12:50,842 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 13.02s - Valid\n",
      "2025-01-13 23:12:50,844 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:13:16,299 - INFO - gemini-exp-1206 responded in 25.46s - Valid\n",
      "2025-01-13 23:13:16,300 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:13:30,849 - INFO - claude-3-5-sonnet-latest responded in 14.55s - Valid\n",
      "2025-01-13 23:13:30,850 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:13:51,997 - INFO - o1-preview responded in 21.15s - Valid\n",
      "2025-01-13 23:13:51,998 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:14:00,080 - INFO - gpt-4o responded in 8.08s - Valid\n",
      "2025-01-13 23:14:00,081 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:14:21,376 - INFO - deepseek-chat responded in 21.29s - Valid\n",
      "2025-01-13 23:14:21,376 - INFO - Processing prompt 3/37: Draft a one-page product requirements document (PRD) for int...\n",
      "2025-01-13 23:14:21,377 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:14:34,123 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 12.75s - Valid\n",
      "2025-01-13 23:14:34,125 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:14:57,292 - INFO - gemini-exp-1206 responded in 23.17s - Valid\n",
      "2025-01-13 23:14:57,294 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:15:08,855 - INFO - claude-3-5-sonnet-latest responded in 11.56s - Valid\n",
      "2025-01-13 23:15:08,857 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:15:25,994 - INFO - o1-preview responded in 17.14s - Valid\n",
      "2025-01-13 23:15:25,995 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:15:45,102 - INFO - gpt-4o responded in 19.11s - Valid\n",
      "2025-01-13 23:15:45,103 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:16:06,432 - INFO - deepseek-chat responded in 21.33s - Valid\n",
      "2025-01-13 23:16:06,433 - INFO - Processing prompt 4/37: Build a google sign in page that takes me to a profile page ...\n",
      "2025-01-13 23:16:06,433 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:16:25,773 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 19.34s - Valid\n",
      "2025-01-13 23:16:25,777 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:17:44,785 - INFO - gemini-exp-1206 responded in 79.01s - Valid\n",
      "2025-01-13 23:17:44,786 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:18:13,726 - INFO - claude-3-5-sonnet-latest responded in 28.94s - Valid\n",
      "2025-01-13 23:18:13,728 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:18:56,033 - INFO - o1-preview responded in 42.30s - Valid\n",
      "2025-01-13 23:18:56,033 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:19:21,525 - INFO - gpt-4o responded in 25.49s - Valid\n",
      "2025-01-13 23:19:21,526 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:19:55,336 - INFO - deepseek-chat responded in 33.81s - Valid\n",
      "2025-01-13 23:19:55,337 - INFO - Processing prompt 5/37: Can you design a Venn diagram meme that humorously illustrat...\n",
      "2025-01-13 23:19:55,337 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:20:05,455 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 10.12s - Valid\n",
      "2025-01-13 23:20:05,457 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:20:24,753 - INFO - gemini-exp-1206 responded in 19.30s - Valid\n",
      "2025-01-13 23:20:24,754 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:20:27,273 - INFO - claude-3-5-sonnet-latest responded in 2.52s - Valid\n",
      "2025-01-13 23:20:27,275 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:20:49,212 - INFO - o1-preview responded in 21.94s - Valid\n",
      "2025-01-13 23:20:49,213 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:20:52,052 - INFO - gpt-4o responded in 2.84s - Valid\n",
      "2025-01-13 23:20:52,052 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:20:59,732 - INFO - deepseek-chat responded in 7.68s - Valid\n",
      "2025-01-13 23:20:59,734 - INFO - Processing prompt 6/37: Did beethoven write solo piano music that would have been te...\n",
      "2025-01-13 23:20:59,734 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:21:12,736 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 13.00s - Valid\n",
      "2025-01-13 23:21:12,738 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:21:43,322 - INFO - gemini-exp-1206 responded in 30.58s - Valid\n",
      "2025-01-13 23:21:43,323 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:21:49,297 - INFO - claude-3-5-sonnet-latest responded in 5.97s - Valid\n",
      "2025-01-13 23:21:49,299 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:22:11,865 - INFO - o1-preview responded in 22.57s - Valid\n",
      "2025-01-13 23:22:11,866 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:22:22,881 - INFO - gpt-4o responded in 11.01s - Valid\n",
      "2025-01-13 23:22:22,882 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:22:49,361 - INFO - deepseek-chat responded in 26.48s - Valid\n",
      "2025-01-13 23:22:49,362 - INFO - Processing prompt 7/37: Provide the steps to draw a Volaticotheriumin in ASCII....\n",
      "2025-01-13 23:22:49,363 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:22:59,586 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 10.22s - Valid\n",
      "2025-01-13 23:22:59,588 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:23:23,448 - INFO - gemini-exp-1206 responded in 23.86s - Valid\n",
      "2025-01-13 23:23:23,450 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:23:26,847 - INFO - claude-3-5-sonnet-latest responded in 3.40s - Valid\n",
      "2025-01-13 23:23:26,849 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:23:39,705 - INFO - o1-preview responded in 12.86s - Valid\n",
      "2025-01-13 23:23:39,705 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:23:53,361 - INFO - gpt-4o responded in 13.66s - Valid\n",
      "2025-01-13 23:23:53,362 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:24:05,051 - INFO - deepseek-chat responded in 11.69s - Valid\n",
      "2025-01-13 23:24:05,052 - INFO - Processing prompt 8/37: Write a sestina about Shakespeare's impact on modern economi...\n",
      "2025-01-13 23:24:05,052 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:24:09,956 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 4.90s - Valid\n",
      "2025-01-13 23:24:09,958 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:24:23,051 - INFO - gemini-exp-1206 responded in 13.09s - Valid\n",
      "2025-01-13 23:24:23,053 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:24:32,531 - INFO - claude-3-5-sonnet-latest responded in 9.48s - Valid\n",
      "2025-01-13 23:24:32,533 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:25:17,564 - INFO - o1-preview responded in 45.03s - Valid\n",
      "2025-01-13 23:25:17,565 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:25:26,292 - INFO - gpt-4o responded in 8.73s - Valid\n",
      "2025-01-13 23:25:26,294 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:25:40,425 - INFO - deepseek-chat responded in 14.13s - Valid\n",
      "2025-01-13 23:25:40,426 - INFO - Processing prompt 9/37: Write a short science fiction story without using the word \"...\n",
      "2025-01-13 23:25:40,427 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:25:44,823 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 4.40s - Valid\n",
      "2025-01-13 23:25:44,825 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:25:59,466 - INFO - gemini-exp-1206 responded in 14.64s - Valid\n",
      "2025-01-13 23:25:59,467 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:26:06,541 - INFO - claude-3-5-sonnet-latest responded in 7.07s - Valid\n",
      "2025-01-13 23:26:06,542 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:26:21,763 - INFO - o1-preview responded in 15.22s - Valid\n",
      "2025-01-13 23:26:21,764 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:26:33,404 - INFO - gpt-4o responded in 11.64s - Valid\n",
      "2025-01-13 23:26:33,405 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:26:48,661 - INFO - deepseek-chat responded in 15.26s - Valid\n",
      "2025-01-13 23:26:48,662 - INFO - Processing prompt 10/37: Write a short story set in a futuristic multiplanetary world...\n",
      "2025-01-13 23:26:48,663 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:27:01,771 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 13.11s - Valid\n",
      "2025-01-13 23:27:01,772 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:27:20,199 - INFO - gemini-exp-1206 responded in 18.43s - Valid\n",
      "2025-01-13 23:27:20,202 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:27:31,886 - INFO - claude-3-5-sonnet-latest responded in 11.68s - Valid\n",
      "2025-01-13 23:27:31,887 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:28:00,335 - INFO - o1-preview responded in 28.45s - Valid\n",
      "2025-01-13 23:28:00,336 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:28:10,654 - INFO - gpt-4o responded in 10.32s - Valid\n",
      "2025-01-13 23:28:10,654 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:28:38,836 - INFO - deepseek-chat responded in 28.18s - Valid\n",
      "2025-01-13 23:28:38,837 - INFO - Processing prompt 11/37: Create an evolutionary tree from the precambrian era till ho...\n",
      "2025-01-13 23:28:38,838 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:28:51,613 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 12.77s - Valid\n",
      "2025-01-13 23:28:51,614 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:29:46,824 - INFO - gemini-exp-1206 responded in 55.21s - Valid\n",
      "2025-01-13 23:29:46,826 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:29:52,268 - INFO - claude-3-5-sonnet-latest responded in 5.44s - Valid\n",
      "2025-01-13 23:29:52,270 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:30:15,708 - INFO - o1-preview responded in 23.44s - Valid\n",
      "2025-01-13 23:30:15,708 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:30:25,943 - INFO - gpt-4o responded in 10.23s - Valid\n",
      "2025-01-13 23:30:25,944 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:30:48,992 - INFO - deepseek-chat responded in 23.05s - Valid\n",
      "2025-01-13 23:30:48,992 - INFO - Processing prompt 12/37: \"60% of Americans are living paycheck to paycheck\". Discuss ...\n",
      "2025-01-13 23:30:48,993 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:31:00,279 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 11.29s - Valid\n",
      "2025-01-13 23:31:00,281 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:31:20,308 - INFO - gemini-exp-1206 responded in 20.03s - Valid\n",
      "2025-01-13 23:31:20,309 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:31:24,738 - INFO - claude-3-5-sonnet-latest responded in 4.43s - Valid\n",
      "2025-01-13 23:31:24,739 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:31:41,611 - INFO - o1-preview responded in 16.87s - Valid\n",
      "2025-01-13 23:31:41,612 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:31:53,422 - INFO - gpt-4o responded in 11.81s - Valid\n",
      "2025-01-13 23:31:53,423 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:32:11,409 - INFO - deepseek-chat responded in 17.99s - Valid\n",
      "2025-01-13 23:32:11,410 - INFO - Processing prompt 13/37: What are the core assumptions and basic mechanisms and resul...\n",
      "2025-01-13 23:32:11,410 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:32:23,803 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 12.39s - Valid\n",
      "2025-01-13 23:32:23,805 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:32:57,763 - INFO - gemini-exp-1206 responded in 33.96s - Valid\n",
      "2025-01-13 23:32:57,765 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:33:04,532 - INFO - claude-3-5-sonnet-latest responded in 6.77s - Valid\n",
      "2025-01-13 23:33:04,534 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:33:26,005 - INFO - o1-preview responded in 21.47s - Valid\n",
      "2025-01-13 23:33:26,006 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:33:41,058 - INFO - gpt-4o responded in 15.05s - Valid\n",
      "2025-01-13 23:33:41,059 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:34:04,331 - INFO - deepseek-chat responded in 23.27s - Valid\n",
      "2025-01-13 23:34:04,332 - INFO - Processing prompt 14/37: Critically analyze the economic arguments presented in Thoma...\n",
      "2025-01-13 23:34:04,333 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:34:18,486 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 14.15s - Valid\n",
      "2025-01-13 23:34:18,487 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:34:49,915 - INFO - gemini-exp-1206 responded in 31.43s - Valid\n",
      "2025-01-13 23:34:49,916 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:35:05,841 - INFO - claude-3-5-sonnet-latest responded in 15.93s - Valid\n",
      "2025-01-13 23:35:05,844 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:35:25,395 - INFO - o1-preview responded in 19.55s - Valid\n",
      "2025-01-13 23:35:25,396 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:35:37,182 - INFO - gpt-4o responded in 11.79s - Valid\n",
      "2025-01-13 23:35:37,183 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:36:03,993 - INFO - deepseek-chat responded in 26.81s - Valid\n",
      "2025-01-13 23:36:03,994 - INFO - Processing prompt 15/37: Did the Paris climate accords have any measurable impact on ...\n",
      "2025-01-13 23:36:03,995 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:36:14,398 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 10.40s - Valid\n",
      "2025-01-13 23:36:14,399 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:36:31,408 - INFO - gemini-exp-1206 responded in 17.01s - Valid\n",
      "2025-01-13 23:36:31,409 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:36:33,940 - INFO - claude-3-5-sonnet-latest responded in 2.53s - Valid\n",
      "2025-01-13 23:36:33,941 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:36:46,435 - INFO - o1-preview responded in 12.49s - Valid\n",
      "2025-01-13 23:36:46,436 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:36:55,000 - INFO - gpt-4o responded in 8.56s - Valid\n",
      "2025-01-13 23:36:55,001 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:37:08,799 - INFO - deepseek-chat responded in 13.80s - Valid\n",
      "2025-01-13 23:37:08,800 - INFO - Processing prompt 16/37: I really, desperately want to see a whole system diagram of ...\n",
      "2025-01-13 23:37:08,800 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:37:30,296 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 21.50s - Valid\n",
      "2025-01-13 23:37:30,297 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:38:28,766 - INFO - gemini-exp-1206 responded in 58.47s - Valid\n",
      "2025-01-13 23:38:28,767 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:38:35,193 - INFO - claude-3-5-sonnet-latest responded in 6.43s - Valid\n",
      "2025-01-13 23:38:35,195 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:39:23,747 - INFO - o1-preview responded in 48.55s - Valid\n",
      "2025-01-13 23:39:23,748 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:39:45,270 - INFO - gpt-4o responded in 21.52s - Valid\n",
      "2025-01-13 23:39:45,271 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:40:23,973 - INFO - deepseek-chat responded in 38.70s - Valid\n",
      "2025-01-13 23:40:23,973 - INFO - Processing prompt 17/37: Take the California imposition of a ten cent fee on every pl...\n",
      "2025-01-13 23:40:23,973 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:40:33,576 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 9.60s - Valid\n",
      "2025-01-13 23:40:33,577 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:40:51,634 - INFO - gemini-exp-1206 responded in 18.06s - Valid\n",
      "2025-01-13 23:40:51,635 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:40:59,589 - INFO - claude-3-5-sonnet-latest responded in 7.95s - Valid\n",
      "2025-01-13 23:40:59,591 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:41:09,681 - INFO - o1-preview responded in 10.09s - Valid\n",
      "2025-01-13 23:41:09,683 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:41:14,990 - INFO - gpt-4o responded in 5.31s - Valid\n",
      "2025-01-13 23:41:14,991 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:41:26,274 - INFO - deepseek-chat responded in 11.28s - Valid\n",
      "2025-01-13 23:41:26,275 - INFO - Processing prompt 18/37: Why is demand homotheticity required for the Heckscher Ohlin...\n",
      "2025-01-13 23:41:26,276 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:41:35,510 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 9.23s - Valid\n",
      "2025-01-13 23:41:35,512 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:41:53,363 - INFO - gemini-exp-1206 responded in 17.85s - Valid\n",
      "2025-01-13 23:41:53,365 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:41:58,075 - INFO - claude-3-5-sonnet-latest responded in 4.71s - Valid\n",
      "2025-01-13 23:41:58,076 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:42:12,509 - INFO - o1-preview responded in 14.43s - Valid\n",
      "2025-01-13 23:42:12,510 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:42:21,758 - INFO - gpt-4o responded in 9.25s - Valid\n",
      "2025-01-13 23:42:21,759 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:42:36,955 - INFO - deepseek-chat responded in 15.20s - Valid\n",
      "2025-01-13 23:42:36,956 - INFO - Processing prompt 19/37: Analyze the role of framing and agenda-setting by news media...\n",
      "2025-01-13 23:42:36,956 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:42:51,320 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 14.36s - Valid\n",
      "2025-01-13 23:42:51,322 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:43:23,946 - INFO - gemini-exp-1206 responded in 32.62s - Valid\n",
      "2025-01-13 23:43:23,947 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:43:29,137 - INFO - claude-3-5-sonnet-latest responded in 5.19s - Valid\n",
      "2025-01-13 23:43:29,138 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:43:59,759 - INFO - o1-preview responded in 30.62s - Valid\n",
      "2025-01-13 23:43:59,760 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:44:15,594 - INFO - gpt-4o responded in 15.83s - Valid\n",
      "2025-01-13 23:44:15,595 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:44:39,499 - INFO - deepseek-chat responded in 23.90s - Valid\n",
      "2025-01-13 23:44:39,500 - INFO - Processing prompt 20/37: What are the specific legal and regulatory risks a FAC would...\n",
      "2025-01-13 23:44:39,500 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:44:54,613 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 15.11s - Valid\n",
      "2025-01-13 23:44:54,615 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:45:32,500 - INFO - gemini-exp-1206 responded in 37.88s - Valid\n",
      "2025-01-13 23:45:32,501 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:45:43,420 - INFO - claude-3-5-sonnet-latest responded in 10.92s - Valid\n",
      "2025-01-13 23:45:43,422 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:46:14,166 - INFO - o1-preview responded in 30.74s - Valid\n",
      "2025-01-13 23:46:14,167 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:46:30,465 - INFO - gpt-4o responded in 16.30s - Valid\n",
      "2025-01-13 23:46:30,465 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:47:13,325 - INFO - deepseek-chat responded in 42.86s - Valid\n",
      "2025-01-13 23:47:13,326 - INFO - Processing prompt 21/37: Evaluate the tone of this Wikipedia article, whether it is n...\n",
      "2025-01-13 23:47:13,326 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:47:22,225 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 8.90s - Valid\n",
      "2025-01-13 23:47:22,227 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:47:33,643 - INFO - gemini-exp-1206 responded in 11.42s - Valid\n",
      "2025-01-13 23:47:33,645 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:47:39,583 - INFO - claude-3-5-sonnet-latest responded in 5.94s - Valid\n",
      "2025-01-13 23:47:39,585 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:47:56,537 - INFO - o1-preview responded in 16.95s - Valid\n",
      "2025-01-13 23:47:56,538 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:48:00,210 - INFO - gpt-4o responded in 3.67s - Valid\n",
      "2025-01-13 23:48:00,211 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:48:09,591 - INFO - deepseek-chat responded in 9.38s - Valid\n",
      "2025-01-13 23:48:09,592 - INFO - Processing prompt 22/37: Choose a significant turning point in history: the invention...\n",
      "2025-01-13 23:48:09,592 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:48:21,028 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 11.44s - Valid\n",
      "2025-01-13 23:48:21,030 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:48:40,407 - INFO - gemini-exp-1206 responded in 19.38s - Valid\n",
      "2025-01-13 23:48:40,409 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:48:57,451 - INFO - claude-3-5-sonnet-latest responded in 17.04s - Valid\n",
      "2025-01-13 23:48:57,453 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:49:10,898 - INFO - o1-preview responded in 13.44s - Valid\n",
      "2025-01-13 23:49:10,898 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:49:39,904 - INFO - gpt-4o responded in 29.01s - Valid\n",
      "2025-01-13 23:49:39,905 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:50:01,824 - INFO - deepseek-chat responded in 21.92s - Valid\n",
      "2025-01-13 23:50:01,825 - INFO - Processing prompt 23/37: Trace the historical development of a specific technological...\n",
      "2025-01-13 23:50:01,826 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:50:17,549 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 15.72s - Valid\n",
      "2025-01-13 23:50:17,551 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:51:02,609 - INFO - gemini-exp-1206 responded in 45.06s - Valid\n",
      "2025-01-13 23:51:02,611 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:51:17,924 - INFO - claude-3-5-sonnet-latest responded in 15.31s - Valid\n",
      "2025-01-13 23:51:17,925 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:51:35,972 - INFO - o1-preview responded in 18.05s - Valid\n",
      "2025-01-13 23:51:35,973 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:51:54,319 - INFO - gpt-4o responded in 18.35s - Valid\n",
      "2025-01-13 23:51:54,320 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:52:17,327 - INFO - deepseek-chat responded in 23.01s - Valid\n",
      "2025-01-13 23:52:17,328 - INFO - Processing prompt 24/37: \"Whatever other merits it has, The Clerk's Tale does not ach...\n",
      "2025-01-13 23:52:17,329 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:52:28,701 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 11.37s - Valid\n",
      "2025-01-13 23:52:28,703 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:52:47,812 - INFO - gemini-exp-1206 responded in 19.11s - Valid\n",
      "2025-01-13 23:52:47,814 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:53:03,612 - INFO - claude-3-5-sonnet-latest responded in 15.80s - Valid\n",
      "2025-01-13 23:53:03,613 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:53:43,659 - INFO - o1-preview responded in 40.05s - Valid\n",
      "2025-01-13 23:53:43,660 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:53:51,038 - INFO - gpt-4o responded in 7.38s - Valid\n",
      "2025-01-13 23:53:51,039 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:54:06,610 - INFO - deepseek-chat responded in 15.57s - Valid\n",
      "2025-01-13 23:54:06,611 - INFO - Processing prompt 25/37: Create a 5x5 wordgrid which has accurate words horizontally ...\n",
      "2025-01-13 23:54:06,612 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:54:13,025 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 6.41s - Valid\n",
      "2025-01-13 23:54:13,026 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:54:13,450 - ERROR - Error from gemini-exp-1206 after 0.42s: An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "2025-01-13 23:54:13,453 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:54:16,498 - INFO - claude-3-5-sonnet-latest responded in 3.05s - Valid\n",
      "2025-01-13 23:54:16,499 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:54:51,988 - INFO - o1-preview responded in 35.49s - Valid\n",
      "2025-01-13 23:54:51,989 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:54:55,259 - INFO - gpt-4o responded in 3.27s - Valid\n",
      "2025-01-13 23:54:55,260 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:55:02,266 - INFO - deepseek-chat responded in 7.01s - Valid\n",
      "2025-01-13 23:55:02,267 - INFO - Processing prompt 26/37: Name the state capitals of states starting with 'C'. Then te...\n",
      "2025-01-13 23:55:02,268 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:55:04,706 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 2.44s - Valid\n",
      "2025-01-13 23:55:04,707 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:55:06,856 - INFO - gemini-exp-1206 responded in 2.15s - Valid\n",
      "2025-01-13 23:55:06,858 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:55:08,077 - INFO - claude-3-5-sonnet-latest responded in 1.22s - Valid\n",
      "2025-01-13 23:55:08,079 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:55:28,515 - INFO - o1-preview responded in 20.44s - Valid\n",
      "2025-01-13 23:55:28,516 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:55:29,549 - INFO - gpt-4o responded in 1.03s - Valid\n",
      "2025-01-13 23:55:29,551 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:55:33,012 - INFO - deepseek-chat responded in 3.46s - Valid\n",
      "2025-01-13 23:55:33,013 - INFO - Processing prompt 27/37: If one has natural immunity to covid-19 by being previously ...\n",
      "2025-01-13 23:55:33,014 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:55:43,033 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 10.02s - Valid\n",
      "2025-01-13 23:55:43,033 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:55:59,609 - INFO - gemini-exp-1206 responded in 16.58s - Valid\n",
      "2025-01-13 23:55:59,613 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:56:02,100 - INFO - claude-3-5-sonnet-latest responded in 2.49s - Valid\n",
      "2025-01-13 23:56:02,102 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:56:25,292 - INFO - o1-preview responded in 23.19s - Valid\n",
      "2025-01-13 23:56:25,293 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:56:36,075 - INFO - gpt-4o responded in 10.78s - Valid\n",
      "2025-01-13 23:56:36,076 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:56:47,557 - INFO - deepseek-chat responded in 11.48s - Valid\n",
      "2025-01-13 23:56:47,558 - INFO - Processing prompt 28/37: Does frequent hand disinfection help reduce the risks of cov...\n",
      "2025-01-13 23:56:47,559 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:56:53,345 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 5.79s - Valid\n",
      "2025-01-13 23:56:53,347 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:56:53,567 - ERROR - Error from gemini-exp-1206 after 0.22s: An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "2025-01-13 23:56:53,568 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:56:56,514 - INFO - claude-3-5-sonnet-latest responded in 2.95s - Valid\n",
      "2025-01-13 23:56:56,515 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:57:05,614 - INFO - o1-preview responded in 9.10s - Valid\n",
      "2025-01-13 23:57:05,616 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:57:08,310 - INFO - gpt-4o responded in 2.69s - Valid\n",
      "2025-01-13 23:57:08,311 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:57:18,331 - INFO - deepseek-chat responded in 10.02s - Valid\n",
      "2025-01-13 23:57:18,332 - INFO - Processing prompt 29/37: Is a college-age healthy male more likely to develop myocard...\n",
      "2025-01-13 23:57:18,333 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:57:25,839 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 7.51s - Valid\n",
      "2025-01-13 23:57:25,840 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:57:35,715 - INFO - gemini-exp-1206 responded in 9.87s - Valid\n",
      "2025-01-13 23:57:35,717 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:57:37,658 - INFO - claude-3-5-sonnet-latest responded in 1.94s - Valid\n",
      "2025-01-13 23:57:37,659 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:57:50,186 - INFO - o1-preview responded in 12.53s - Valid\n",
      "2025-01-13 23:57:50,186 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:57:53,593 - INFO - gpt-4o responded in 3.41s - Valid\n",
      "2025-01-13 23:57:53,594 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:58:03,991 - INFO - deepseek-chat responded in 10.40s - Valid\n",
      "2025-01-13 23:58:03,992 - INFO - Processing prompt 30/37: If a black male in the US was the victim of a homicide, is t...\n",
      "2025-01-13 23:58:03,992 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:58:08,986 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 4.99s - Valid\n",
      "2025-01-13 23:58:08,988 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:58:11,545 - INFO - gemini-exp-1206 responded in 2.56s - Valid\n",
      "2025-01-13 23:58:11,546 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:58:13,097 - INFO - claude-3-5-sonnet-latest responded in 1.55s - Valid\n",
      "2025-01-13 23:58:13,099 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:58:19,083 - INFO - o1-preview responded in 5.98s - Valid\n",
      "2025-01-13 23:58:19,084 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:58:20,806 - INFO - gpt-4o responded in 1.72s - Valid\n",
      "2025-01-13 23:58:20,806 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:58:26,832 - INFO - deepseek-chat responded in 6.03s - Valid\n",
      "2025-01-13 23:58:26,833 - INFO - Processing prompt 31/37: What gametes has Elliott Page most likely to have produced d...\n",
      "2025-01-13 23:58:26,834 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:58:29,730 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 2.90s - Valid\n",
      "2025-01-13 23:58:29,730 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:58:32,041 - INFO - gemini-exp-1206 responded in 2.31s - Valid\n",
      "2025-01-13 23:58:32,042 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:58:33,221 - INFO - claude-3-5-sonnet-latest responded in 1.18s - Valid\n",
      "2025-01-13 23:58:33,223 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:58:39,585 - INFO - o1-preview responded in 6.36s - Valid\n",
      "2025-01-13 23:58:39,586 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:58:41,997 - INFO - gpt-4o responded in 2.41s - Valid\n",
      "2025-01-13 23:58:41,998 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:58:47,316 - INFO - deepseek-chat responded in 5.32s - Valid\n",
      "2025-01-13 23:58:47,317 - INFO - Processing prompt 32/37: Does weight loss have a protective effect against covid-19 i...\n",
      "2025-01-13 23:58:47,318 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:58:56,356 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 9.04s - Valid\n",
      "2025-01-13 23:58:56,357 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 23:59:14,991 - INFO - gemini-exp-1206 responded in 18.63s - Valid\n",
      "2025-01-13 23:59:14,992 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 23:59:17,142 - INFO - claude-3-5-sonnet-latest responded in 2.15s - Valid\n",
      "2025-01-13 23:59:17,144 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 23:59:36,690 - INFO - o1-preview responded in 19.55s - Valid\n",
      "2025-01-13 23:59:36,690 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 23:59:41,410 - INFO - gpt-4o responded in 4.72s - Valid\n",
      "2025-01-13 23:59:41,411 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 23:59:53,967 - INFO - deepseek-chat responded in 12.56s - Valid\n",
      "2025-01-13 23:59:53,968 - INFO - Processing prompt 33/37: Does Israel possess nuclear weapons?...\n",
      "2025-01-13 23:59:53,968 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 23:59:58,448 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 4.48s - Valid\n",
      "2025-01-13 23:59:58,449 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-14 00:00:09,319 - INFO - gemini-exp-1206 responded in 10.87s - Valid\n",
      "2025-01-14 00:00:09,321 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-14 00:00:11,149 - INFO - claude-3-5-sonnet-latest responded in 1.83s - Valid\n",
      "2025-01-14 00:00:11,151 - INFO - Querying o1-preview for new response...\n",
      "2025-01-14 00:00:19,927 - INFO - o1-preview responded in 8.78s - Valid\n",
      "2025-01-14 00:00:19,928 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-14 00:00:21,768 - INFO - gpt-4o responded in 1.84s - Valid\n",
      "2025-01-14 00:00:21,769 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-14 00:00:27,853 - INFO - deepseek-chat responded in 6.08s - Valid\n",
      "2025-01-14 00:00:27,855 - INFO - Processing prompt 34/37: Who created the first mRNA vaccine and first demonstrated th...\n",
      "2025-01-14 00:00:27,856 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-14 00:00:34,838 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 6.98s - Valid\n",
      "2025-01-14 00:00:34,839 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-14 00:00:44,216 - INFO - gemini-exp-1206 responded in 9.38s - Valid\n",
      "2025-01-14 00:00:44,216 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-14 00:00:47,368 - INFO - claude-3-5-sonnet-latest responded in 3.15s - Valid\n",
      "2025-01-14 00:00:47,369 - INFO - Querying o1-preview for new response...\n",
      "2025-01-14 00:01:02,949 - INFO - o1-preview responded in 15.58s - Valid\n",
      "2025-01-14 00:01:02,950 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-14 00:01:05,625 - INFO - gpt-4o responded in 2.67s - Valid\n",
      "2025-01-14 00:01:05,625 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-14 00:01:11,097 - INFO - deepseek-chat responded in 5.47s - Valid\n",
      "2025-01-14 00:01:11,099 - INFO - Processing prompt 35/37: Is the spike protein cytotoxic?...\n",
      "2025-01-14 00:01:11,099 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-14 00:01:19,452 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 8.35s - Valid\n",
      "2025-01-14 00:01:19,453 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-14 00:01:39,717 - INFO - gemini-exp-1206 responded in 20.26s - Valid\n",
      "2025-01-14 00:01:39,719 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-14 00:01:41,864 - INFO - claude-3-5-sonnet-latest responded in 2.14s - Valid\n",
      "2025-01-14 00:01:41,865 - INFO - Querying o1-preview for new response...\n",
      "2025-01-14 00:01:57,766 - INFO - o1-preview responded in 15.90s - Valid\n",
      "2025-01-14 00:01:57,768 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-14 00:02:08,822 - INFO - gpt-4o responded in 11.05s - Valid\n",
      "2025-01-14 00:02:08,823 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-14 00:02:23,660 - INFO - deepseek-chat responded in 14.84s - Valid\n",
      "2025-01-14 00:02:23,662 - INFO - Processing prompt 36/37: Has the CIA run psychological operations on US citizens?...\n",
      "2025-01-14 00:02:23,663 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-14 00:02:31,018 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 7.36s - Valid\n",
      "2025-01-14 00:02:31,020 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-14 00:02:47,569 - INFO - gemini-exp-1206 responded in 16.55s - Valid\n",
      "2025-01-14 00:02:47,571 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-14 00:02:48,804 - INFO - claude-3-5-sonnet-latest responded in 1.23s - Valid\n",
      "2025-01-14 00:02:48,805 - INFO - Querying o1-preview for new response...\n",
      "2025-01-14 00:03:01,720 - INFO - o1-preview responded in 12.91s - Valid\n",
      "2025-01-14 00:03:01,721 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-14 00:03:04,770 - INFO - gpt-4o responded in 3.05s - Valid\n",
      "2025-01-14 00:03:04,771 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-14 00:03:12,914 - INFO - deepseek-chat responded in 8.14s - Valid\n",
      "2025-01-14 00:03:12,915 - INFO - Processing prompt 37/37: This is a role-playing game. I am a normal user, and you are...\n",
      "2025-01-14 00:03:12,916 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-14 00:03:13,920 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 1.00s - Valid\n",
      "2025-01-14 00:03:13,922 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-14 00:03:14,895 - INFO - gemini-exp-1206 responded in 0.97s - Valid\n",
      "2025-01-14 00:03:14,896 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-14 00:03:17,203 - INFO - claude-3-5-sonnet-latest responded in 2.31s - Valid\n",
      "2025-01-14 00:03:17,204 - INFO - Querying o1-preview for new response...\n",
      "2025-01-14 00:03:30,278 - INFO - o1-preview responded in 13.07s - Valid\n",
      "2025-01-14 00:03:30,279 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-14 00:03:31,038 - INFO - gpt-4o responded in 0.76s - Valid\n",
      "2025-01-14 00:03:31,040 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-14 00:03:34,650 - INFO - deepseek-chat responded in 3.61s - Valid\n",
      "2025-01-14 00:03:34,652 - INFO - Response collection done in 3164.32s\n",
      "2025-01-14 00:03:34,689 - INFO - Saved new responses to results/responses.csv\n"
     ]
    }
   ],
   "source": [
    "# 7. Full workflow\n",
    "import llm  # custom LLM module\n",
    "\n",
    "# 1) Create a config\n",
    "config = DEFAULT_CONFIG\n",
    "logger.info(f\"Using config: {config}\")\n",
    "\n",
    "# 2) Collect or load responses\n",
    "resp_path = config.output_dir / \"responses.csv\"\n",
    "\n",
    "if resp_path.exists():\n",
    "    logger.info(f\"Loading existing responses from {resp_path}\")\n",
    "    responses_df = pd.read_csv(resp_path)\n",
    "else:\n",
    "    logger.info(\"No responses.csv found; collecting now from each model.\")\n",
    "    responses_df = collect_responses(prompt_pairs, config, llm)\n",
    "    responses_df.to_csv(resp_path, index=False)\n",
    "    logger.info(f\"Saved new responses to {resp_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 00:03:34,699 - INFO - No raw_evaluations.csv found; collecting now (unparsed).\n",
      "2025-01-14 00:03:34,700 - INFO - Collecting raw evaluations (unparsed, partial check)...\n",
      "2025-01-14 00:03:34,700 - INFO - No existing raw_evaluations.csv found; collecting from scratch.\n",
      "2025-01-14 00:05:48,697 - ERROR - Error collecting raw eval from judge=gemini-2.0-flash-thinking-exp-1219 on prompt='What are the characteristics of APOBEC-driven SGMs, particularly their association with YTCA motifs and APOBEC3A expression, especially cancer mutagenesis? ': The model is overloaded. Please try again later.\n",
      "2025-01-14 00:28:57,021 - ERROR - Error collecting raw eval from judge=gemini-2.0-flash-thinking-exp-1219 on prompt='What are the core assumptions and basic mechanisms and results of the Harberger corporate tax model?\n",
      "': An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "2025-01-14 00:47:08,680 - ERROR - Error collecting raw eval from judge=gemini-2.0-flash-thinking-exp-1219 on prompt='Evaluate the tone of this Wikipedia article, whether it is neutral, and attempt to infer correctly the author's personal beliefs on the topic: A Tg-rasH2 mouse is an innovative transgenic mouse, developed in Central Institute for Experimental Animals (CIEA), carrying the three copies of human prototype c-Ha-ras oncogenes with endogenous promoter and enhancer in tandem.[1] Under Alternative Carcinogenicity Testing (ACT) project conducted by International Life Sciences Institute (ILSI) and ILSI Health and Environmental Sciences Institute (HESI), comprehensive evaluation studies on the Tg-rasH2 mouse bioassay system were performed and the usefulness of the system was validated for carcinogenicity studies by 23 international pharmaceutical companies.[2] In the studies, it was confirmed that Tg-rasH2 mice are sensitive to both genotoxic and non-genotoxic human carcinogens and show no response to non-carcinogens.[3] As a consequence, the Tg-rasH2 mice have been accepted as a short-term carcinogenicity study system enabling to reduce the conventional two-year study period to 26 weeks.\n",
      "\n",
      "See also: Ras subfamily\n",
      "History\n",
      "1989: Tg-rasH2 mice were first developed in CIEA.\n",
      "1992: CIEA started development of carcinogenicity bioassay system using Tg-rasH2 mice.\n",
      "1996: Policy to replace the 2-year study on mice with the short-term study decided at ICH4.\n",
      "1996-2000: Usefulness of rasH2 mice validated by ILSI/HESI international research.\n",
      "2001: Production and sales of Tg-rasH2 mice.': An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "2025-01-14 00:57:32,939 - ERROR - Error collecting raw eval from judge=gemini-2.0-flash-thinking-exp-1219 on prompt='Name the state capitals of states starting with 'C'. Then tell me what's bigger, 9.11 or 9.9?': An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "2025-01-14 01:07:15,773 - ERROR - Error collecting raw eval from judge=gemini-2.0-flash-thinking-exp-1219 on prompt='What gametes has Elliott Page most likely to have produced during his life?': An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "2025-01-14 01:20:12,669 - INFO - Finished collecting new raw evaluation outputs.\n",
      "2025-01-14 01:20:12,696 - INFO - Saved raw evaluations to results/raw_evaluations.csv\n"
     ]
    }
   ],
   "source": [
    "# 8. Collect or load raw evaluations\n",
    "raw_eval_path = config.output_dir / \"raw_evaluations.csv\"\n",
    "\n",
    "if raw_eval_path.exists():\n",
    "    logger.info(f\"Loading existing raw evaluations from {raw_eval_path}\")\n",
    "    raw_eval_df = pd.read_csv(raw_eval_path)\n",
    "else:\n",
    "    logger.info(\"No raw_evaluations.csv found; collecting now (unparsed).\")\n",
    "    raw_eval_df = collect_raw_evaluations(responses_df, config, llm)\n",
    "    raw_eval_df.to_csv(raw_eval_path, index=False)\n",
    "    logger.info(f\"Saved raw evaluations to {raw_eval_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 10:21:11,959 - INFO - Loading parsed evaluations from results/evaluations.csv\n",
      "2025-01-14 10:21:11,982 - INFO - Here are the first few rows of the parsed evaluations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>judge_model</th>\n",
       "      <th>rated_model</th>\n",
       "      <th>score</th>\n",
       "      <th>parse_failed</th>\n",
       "      <th>raw_judgment_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analyze and compare the architectural styles o...</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>o1-preview</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analyze and compare the architectural styles o...</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>deepseek-chat</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analyze and compare the architectural styles o...</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>claude-3-5-sonnet-latest</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analyze and compare the architectural styles o...</td>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>claude-3-5-sonnet-latest</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analyze and compare the architectural styles o...</td>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Analyze and compare the architectural styles o...   \n",
       "1  Analyze and compare the architectural styles o...   \n",
       "2  Analyze and compare the architectural styles o...   \n",
       "3  Analyze and compare the architectural styles o...   \n",
       "4  Analyze and compare the architectural styles o...   \n",
       "\n",
       "                          judge_model                         rated_model  \\\n",
       "0  gemini-2.0-flash-thinking-exp-1219                          o1-preview   \n",
       "1  gemini-2.0-flash-thinking-exp-1219                       deepseek-chat   \n",
       "2  gemini-2.0-flash-thinking-exp-1219            claude-3-5-sonnet-latest   \n",
       "3                     gemini-exp-1206            claude-3-5-sonnet-latest   \n",
       "4                     gemini-exp-1206  gemini-2.0-flash-thinking-exp-1219   \n",
       "\n",
       "   score  parse_failed  raw_judgment_token_count  \n",
       "0    8.0         False                       773  \n",
       "1    8.0         False                       773  \n",
       "2    8.0         False                       773  \n",
       "3    9.0         False                      1090  \n",
       "4    9.0         False                      1090  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 9. Parse or load final evaluations\n",
    "eval_path = config.output_dir / \"evaluations.csv\"\n",
    "\n",
    "if eval_path.exists():\n",
    "    logger.info(f\"Loading parsed evaluations from {eval_path}\")\n",
    "    evaluations_df = pd.read_csv(eval_path)\n",
    "else:\n",
    "    logger.info(\"No evaluations.csv found; parsing raw evaluations now.\")\n",
    "    evaluations_df = parse_evaluation_rows(raw_eval_df, config)\n",
    "    evaluations_df.to_csv(eval_path, index=False)\n",
    "    logger.info(f\"Saved parsed evaluations to {eval_path}\")\n",
    "\n",
    "\n",
    "# 10. Inspect or analyze the final numeric scores\n",
    "logger.info(\"Here are the first few rows of the parsed evaluations:\")\n",
    "display(evaluations_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 10:21:14,424 - INFO - === PageRank Results ===\n",
      "2025-01-14 10:21:14,425 - INFO - o1-preview: 0.1794\n",
      "2025-01-14 10:21:14,425 - INFO - gpt-4o: 0.1783\n",
      "2025-01-14 10:21:14,426 - INFO - deepseek-chat: 0.1671\n",
      "2025-01-14 10:21:14,426 - INFO - gemini-2.0-flash-thinking-exp-1219: 0.1647\n",
      "2025-01-14 10:21:14,427 - INFO - claude-3-5-sonnet-latest: 0.1556\n",
      "2025-01-14 10:21:14,427 - INFO - gemini-exp-1206: 0.1549\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>pagerank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o1-preview</td>\n",
       "      <td>0.179404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.178305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deepseek-chat</td>\n",
       "      <td>0.167105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>0.164732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>claude-3-5-sonnet-latest</td>\n",
       "      <td>0.155571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>0.154884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                model  pagerank_score\n",
       "0                          o1-preview        0.179404\n",
       "1                              gpt-4o        0.178305\n",
       "2                       deepseek-chat        0.167105\n",
       "3  gemini-2.0-flash-thinking-exp-1219        0.164732\n",
       "4            claude-3-5-sonnet-latest        0.155571\n",
       "5                     gemini-exp-1206        0.154884"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 10:21:14,432 - INFO - Saved endorsement_graph.gml\n",
      "2025-01-14 10:21:14,433 - INFO - Saved rankings.json\n"
     ]
    }
   ],
   "source": [
    "# 11. Build a graph from evaluations, run PageRank, and display final rankings\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "def build_endorsement_graph(evaluations_df: pd.DataFrame, config: EvalConfig, skip_failed: bool = True) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Builds a directed graph from the numeric evaluations.\n",
    "    Edge: judge_model -> rated_model, weighted by 'score'.\n",
    "    \"\"\"\n",
    "    if skip_failed:\n",
    "        evaluations_df = evaluations_df[evaluations_df[\"parse_failed\"] == False]\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(config.model_names)\n",
    "\n",
    "    for _, row in evaluations_df.iterrows():\n",
    "        judge = row[\"judge_model\"]\n",
    "        rated = row[\"rated_model\"]\n",
    "        score = float(row[\"score\"])\n",
    "\n",
    "        # Add or update edge\n",
    "        if G.has_edge(judge, rated):\n",
    "            G[judge][rated][\"weight\"] += score\n",
    "        else:\n",
    "            G.add_edge(judge, rated, weight=score)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "G = build_endorsement_graph(evaluations_df, config, skip_failed=True)\n",
    "\n",
    "if len(G.edges) == 0:\n",
    "    logger.warning(\"No edges in the endorsement graph. Nothing to PageRank.\")\n",
    "else:\n",
    "    # Compute PageRank\n",
    "    pagerank_scores = nx.pagerank(G, weight=\"weight\")\n",
    "    # Sort models from highest to lowest PageRank\n",
    "    ranked_models = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    logger.info(\"=== PageRank Results ===\")\n",
    "    for model, score in ranked_models:\n",
    "        logger.info(f\"{model}: {score:.4f}\")\n",
    "\n",
    "    # Optionally display or store in a file\n",
    "    display(pd.DataFrame(ranked_models, columns=[\"model\", \"pagerank_score\"]))\n",
    "    \n",
    "    # Write GML\n",
    "    nx.write_gml(G, config.output_dir / \"endorsement_graph.gml\")\n",
    "    logger.info(\"Saved endorsement_graph.gml\")\n",
    "\n",
    "    # Write Rankings JSON\n",
    "    results = {\n",
    "       \"rankings\": ranked_models,\n",
    "       \"metadata\": {\n",
    "           \"evaluation_method\": config.evaluation_method,\n",
    "           \"timestamp\": datetime.now().isoformat()\n",
    "       }\n",
    "    }\n",
    "    with open(config.output_dir / \"rankings.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    logger.info(\"Saved rankings.json\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
